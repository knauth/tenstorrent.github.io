# Multi-Head Attention

Multi-Head Attention is an important part of all Transformer-based models. This tutorial will show how to write it and how to then optimize it.

## Enable program cache

## Write Multi-Head Attention using ttnn

Multi-head can be implemented in `torch` using just 6 operations:

1. `torch.matmul`
2. `torch.add`
3. `torch.reshape`
4. `torch.permute`
5. `torch.mul`
6. `torch.softmax`

`ttnn` provides the exact same APIs to do that and therefore multi-head attention can be implemented in a very similar fashion. Except, when using `ttnn`, the user should be mindful of the tensor layout.

Now that the model is written, letâ€™s create input tensors to run it and test it

## Configuration

## Initialize activations and weights using torch

## Convert activations and weights to ttnn

## Run the first iteration of Multi-Head Attention

## Run a subsequent iteration of Multi-Head Attention

## Write optimized version of Multi-Head Attention

Optimized version of the multi-head attention can be written by:

- Tilizing all of the tensors ahead of time
- Using more performant matmuls that fuse bias and specify the number of cores they execute on
- Putting every tensor into L1
- Using bfloat8_b data_type
- Using custom `ttnn.transformer` operations instead of `ttnn.permute` and `ttnn.reshape`

`ttnn.deallocate` calls are needed because otherwise, the cores on the device will run out of the L1 memory

## Pre-process the parameters of the optimized model

1. Fuse QKV weights and biases
2. Reshape and tilize for the optimized operations using preprocess_linear_weight and preprocess_linear_bias
3. Move to device

## Run the first iteration of the optimized Multi-Head Attention

## Run a subsequent iteration of the optimized Multi-Head Attention

Note that the optimized multi-head attention is 2 orders of magnitude faster than the initial version

## Check that the output of the optimized version matches the output of the original implementation

## Close the device
