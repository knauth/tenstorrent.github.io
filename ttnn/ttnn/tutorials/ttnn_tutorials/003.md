# Multi-Head Attention

Multi-Head Attention is an important part of all Transformer-based models. This tutorial will show how to write it and how to then optimize it.

```ipython3
import time
import torch
import ttnn

torch.manual_seed(0)

device_id = 0
device = ttnn.open_device(device_id=device_id, l1_small_size=8192)
```

<pre>
2024-04-18 18:42:42.615 | DEBUG    | ttnn:&lt;module&gt;:126 - Initial ttnn.CONFIG:
{&#39;cache_path&#39;: PosixPath(&#39;/home/ubuntu/.cache/ttnn&#39;),
 &#39;comparison_mode_pcc&#39;: 0.9999,
 &#39;enable_comparison_mode&#39;: False,
 &#39;enable_detailed_buffer_report&#39;: False,
 &#39;enable_detailed_tensor_report&#39;: False,
 &#39;enable_fast_runtime_mode&#39;: False,
 &#39;enable_graph_report&#39;: False,
 &#39;enable_logging&#39;: False,
 &#39;enable_model_cache&#39;: False,
 &#39;model_cache_path&#39;: PosixPath(&#39;/home/ubuntu/.cache/ttnn/models&#39;),
 &#39;report_name&#39;: None,
 &#39;root_report_path&#39;: PosixPath(&#39;generated/ttnn/reports&#39;),
 &#39;throw_exception_on_fallback&#39;: False,
 &#39;tmp_dir&#39;: PosixPath(&#39;/tmp/ttnn&#39;)}
</pre>

<pre>
<span style="color: rgb(0,128,0)">                 Device</span> | <span class="ansi-bold" style="color: rgb(100,149,237)">INFO    </span> | Opening user mode device driver
<span class="ansi-green-fg">2024-04-18 18:42:42.906</span> | <span class="ansi-bold" style="color: rgb(100,149,237)">INFO    </span> | <span class="ansi-cyan-fg">SiliconDriver  </span> - Detected 1 PCI device : {0}
<span class="ansi-green-fg">2024-04-18 18:42:42.918</span> | <span class="ansi-bold" style="color: rgb(255,165,0)">WARNING </span> | <span class="ansi-cyan-fg">SiliconDriver  </span> - init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:07:00.0)
<span class="ansi-green-fg">2024-04-18 18:42:42.918</span> | <span class="ansi-bold" style="color: rgb(255,165,0)">WARNING </span> | <span class="ansi-cyan-fg">SiliconDriver  </span> - Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:07:00.0)
<span class="ansi-green-fg">2024-04-18 18:42:42.920</span> | <span class="ansi-bold" style="color: rgb(255,165,0)">WARNING </span> | <span class="ansi-cyan-fg">SiliconDriver  </span> - bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind.
<span class="ansi-yellow-fg">---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device-&gt;Host perf (Issue #893).
</span><span style="color: rgb(0,128,0)">                  Metal</span> | <span class="ansi-bold" style="color: rgb(100,149,237)">INFO    </span> | Initializing device 0. Program cache is NOT enabled
<span style="color: rgb(0,128,0)">                  Metal</span> | <span class="ansi-bold" style="color: rgb(100,149,237)">INFO    </span> | AI CLK for device 0 is:   1202 MHz
</pre>

## Enable program cache

```ipython3
ttnn.enable_program_cache(device)
```

<pre>
<span style="color: rgb(0,128,0)">                  Metal</span> | <span class="ansi-bold" style="color: rgb(100,149,237)">INFO    </span> | Enabling program cache on device 0
</pre>

## Write Multi-Head Attention using ttnn

Multi-head can be implemented in `torch` using just 6 operations:

1. `torch.matmul`
2. `torch.add`
3. `torch.reshape`
4. `torch.permute`
5. `torch.mul`
6. `torch.softmax`

`ttnn` provides the exact same APIs to do that and therefore multi-head attention can be implemented in a very similar fashion. Except, when using `ttnn`, the user should be mindful of the tensor layout.

```ipython3
def multi_head_attention(
    hidden_states,
    attention_mask,
    query_weight,
    query_bias,
    key_weight,
    key_bias,
    value_weight,
    value_bias,
    output_weight,
    output_bias,
    *,
    num_heads,
):
    fallback_reshape = ttnn.get_fallback_function(ttnn.reshape)

    batch_size, sequence_size, hidden_size = hidden_states.shape
    head_size = hidden_size // num_heads

    query = hidden_states @ query_weight
    query = query + query_bias
    query = ttnn.to_layout(query, layout=ttnn.ROW_MAJOR_LAYOUT)
    query = fallback_reshape(query, (batch_size, sequence_size, num_heads, head_size))
    query = ttnn.to_layout(query, layout=ttnn.TILE_LAYOUT)
    query = ttnn.permute(query, (0, 2, 1, 3))

    key = hidden_states @ key_weight
    key = key + key_bias
    key = ttnn.to_layout(key, layout=ttnn.ROW_MAJOR_LAYOUT)
    key = fallback_reshape(key, (batch_size, sequence_size, num_heads, head_size))
    key = ttnn.to_layout(key, layout=ttnn.TILE_LAYOUT)
    key = ttnn.permute(key, (0, 2, 3, 1))

    value = hidden_states @ value_weight
    value = value + value_bias
    value = ttnn.to_layout(value, layout=ttnn.ROW_MAJOR_LAYOUT)
    value = fallback_reshape(value, (batch_size, sequence_size, num_heads, head_size))
    value = ttnn.to_layout(value, layout=ttnn.TILE_LAYOUT)
    value = ttnn.permute(value, (0, 2, 1, 3))

    attention_scores = query @ key
    attention_scores = attention_scores * (1 / (head_size**0.5))
    attention_scores += attention_mask
    attention_probs = ttnn.softmax(attention_scores, dim=-1)

    context_layer = attention_probs @ value
    context_layer = ttnn.permute(context_layer, (0, 2, 1, 3))
    context_layer = ttnn.to_layout(context_layer, layout=ttnn.ROW_MAJOR_LAYOUT)
    context_layer = fallback_reshape(context_layer, (batch_size, sequence_size, hidden_size))
    context_layer = ttnn.to_layout(context_layer, layout=ttnn.TILE_LAYOUT)

    self_output = context_layer @ output_weight
    self_output = self_output + output_bias

    return self_output
```

Now that the model is written, letâ€™s create input tensors to run it and test it

## Configuration

```ipython3
batch_size = 8
sequence_size = 384
num_heads = 16
head_size = 64
hidden_size = num_heads * head_size
```

## Initialize activations and weights using torch

```ipython3
torch_hidden_states = torch.randn((batch_size, sequence_size, hidden_size), dtype=torch.bfloat16)
torch_attention_mask = torch.randn((batch_size, 1, 1, sequence_size), dtype=torch.bfloat16)
torch_query_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)
torch_query_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)
torch_key_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)
torch_key_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)
torch_value_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)
torch_value_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)
torch_output_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)
torch_output_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)
```

## Convert activations and weights to ttnn

```ipython3
hidden_states = ttnn.from_torch(torch_hidden_states, layout=ttnn.TILE_LAYOUT, device=device)
attention_mask = ttnn.from_torch(torch_attention_mask, layout=ttnn.TILE_LAYOUT, device=device)
query_weight = ttnn.from_torch(torch_query_weight, layout=ttnn.TILE_LAYOUT, device=device)
query_bias = ttnn.from_torch(torch_query_bias, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)
key_weight = ttnn.from_torch(torch_key_weight, layout=ttnn.TILE_LAYOUT, device=device)
key_bias = ttnn.from_torch(torch_key_bias, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)
value_weight = ttnn.from_torch(torch_value_weight, layout=ttnn.TILE_LAYOUT, device=device)
value_bias = ttnn.from_torch(torch_value_bias, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)
output_weight = ttnn.from_torch(torch_output_weight, layout=ttnn.TILE_LAYOUT, device=device)
output_bias = ttnn.from_torch(torch_output_bias, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)
```

## Run the first iteration of Multi-Head Attention

```ipython3
start = time.time()
multi_head_attention(
    hidden_states,
    attention_mask,
    query_weight,
    query_bias,
    key_weight,
    key_bias,
    value_weight,
    value_bias,
    output_weight,
    output_bias,
    num_heads=num_heads,
)
end = time.time()
duration = end - start
```

<pre>
2024-04-18 18:42:45.752 | WARNING  | ttnn.decorators:call_wrapper:557 - ttnn.reshape: falling back to CPU due to TT_THROW @ ttnn/cpp/ttnn/operations/core.hpp:98: tt::exception
info:
Unable to reshape given tensor!
</pre>

<pre>
<span style="color: rgb(0,128,0)">                 Always</span> | <span class="ansi-bold" style="color: rgb(255,0,0)">FATAL   </span> | Unable to reshape given tensor!
</pre>

<pre>
2024-04-18 18:42:47.240 | WARNING  | ttnn.decorators:call_wrapper:557 - ttnn.reshape: falling back to CPU due to TT_THROW @ ttnn/cpp/ttnn/operations/core.hpp:98: tt::exception
info:
Unable to reshape given tensor!
</pre>

<pre>
<span style="color: rgb(0,128,0)">                 Always</span> | <span class="ansi-bold" style="color: rgb(255,0,0)">FATAL   </span> | Unable to reshape given tensor!
</pre>

<pre>
2024-04-18 18:42:47.605 | WARNING  | ttnn.decorators:call_wrapper:557 - ttnn.reshape: falling back to CPU due to TT_THROW @ ttnn/cpp/ttnn/operations/core.hpp:98: tt::exception
info:
Unable to reshape given tensor!
</pre>

<pre>
<span style="color: rgb(0,128,0)">                 Always</span> | <span class="ansi-bold" style="color: rgb(255,0,0)">FATAL   </span> | Unable to reshape given tensor!
</pre>

<pre>
2024-04-18 18:42:50.150 | WARNING  | ttnn.decorators:call_wrapper:557 - ttnn.reshape: falling back to CPU due to TT_THROW @ ttnn/cpp/ttnn/operations/core.hpp:98: tt::exception
info:
Unable to reshape given tensor!
</pre>

<pre>
<span style="color: rgb(0,128,0)">                 Always</span> | <span class="ansi-bold" style="color: rgb(255,0,0)">FATAL   </span> | Unable to reshape given tensor!
</pre>

```ipython3
print(f"Multi-head attention ran in {duration} seconds for the first iteration")
```

<pre>
Multi-head attention ran in 6.2719199657440186 seconds for the first iteration
</pre>

## Run a subsequent iteration of Multi-Head Attention

```ipython3
start = time.time()
output = multi_head_attention(
    hidden_states,
    attention_mask,
    query_weight,
    query_bias,
    key_weight,
    key_bias,
    value_weight,
    value_bias,
    output_weight,
    output_bias,
    num_heads=num_heads,
)
end = time.time()
duration = end - start
```

<pre>
2024-04-18 18:42:50.786 | WARNING  | ttnn.decorators:call_wrapper:557 - ttnn.reshape: falling back to CPU due to TT_THROW @ ttnn/cpp/ttnn/operations/core.hpp:98: tt::exception
info:
Unable to reshape given tensor!
2024-04-18 18:42:50.813 | WARNING  | ttnn.decorators:call_wrapper:557 - ttnn.reshape: falling back to CPU due to TT_THROW @ ttnn/cpp/ttnn/operations/core.hpp:98: tt::exception
info:
Unable to reshape given tensor!
</pre>

<pre>
<span style="color: rgb(0,128,0)">                 Always</span> | <span class="ansi-bold" style="color: rgb(255,0,0)">FATAL   </span> | Unable to reshape given tensor!
<span style="color: rgb(0,128,0)">                 Always</span> | <span class="ansi-bold" style="color: rgb(255,0,0)">FATAL   </span> | Unable to reshape given tensor!
</pre>

<pre>
2024-04-18 18:42:51.171 | WARNING  | ttnn.decorators:call_wrapper:557 - ttnn.reshape: falling back to CPU due to TT_THROW @ ttnn/cpp/ttnn/operations/core.hpp:98: tt::exception
info:
Unable to reshape given tensor!
</pre>

<pre>
<span style="color: rgb(0,128,0)">                 Always</span> | <span class="ansi-bold" style="color: rgb(255,0,0)">FATAL   </span> | Unable to reshape given tensor!
</pre>

<pre>
2024-04-18 18:42:51.535 | WARNING  | ttnn.decorators:call_wrapper:557 - ttnn.reshape: falling back to CPU due to TT_THROW @ ttnn/cpp/ttnn/operations/core.hpp:98: tt::exception
info:
Unable to reshape given tensor!
</pre>

<pre>
<span style="color: rgb(0,128,0)">                 Always</span> | <span class="ansi-bold" style="color: rgb(255,0,0)">FATAL   </span> | Unable to reshape given tensor!
</pre>

```ipython3
print(f"Multi-head attention ran in {duration} seconds for the subsequent iteration because of the program cache")
```

<pre>
Multi-head attention ran in 1.3269269466400146 seconds for the subsequent iteration because of the program cache
</pre>

## Write optimized version of Multi-Head Attention

Optimized version of the multi-head attention can be written by:

- Tilizing all of the tensors ahead of time
- Using more performant matmuls that fuse bias and specify the number of cores they execute on
- Putting every tensor into L1
- Using bfloat8_b data_type
- Using custom `ttnn.transformer` operations instead of `ttnn.permute` and `ttnn.reshape`

`ttnn.deallocate` calls are needed because otherwise, the cores on the device will run out of the L1 memory

```ipython3
def optimized_multi_head_attention(
    hidden_states,
    attention_mask,
    fused_qkv_weight,
    fused_qkv_bias,
    self_output_weight,
    self_output_bias,
    *,
    num_heads,
    num_cores_x=12,
):
    batch_size, _, hidden_size = hidden_states.shape
    head_size = hidden_size // num_heads

    hidden_states = ttnn.to_layout(hidden_states, ttnn.TILE_LAYOUT)

    fused_qkv_output = ttnn.linear(
        hidden_states,
        fused_qkv_weight,
        bias=fused_qkv_bias,
        memory_config=ttnn.L1_MEMORY_CONFIG,
        dtype=ttnn.bfloat8_b,
        core_grid=ttnn.CoreGrid(y=batch_size, x=num_cores_x),
    )

    (
        query,
        key,
        value,
    ) = ttnn.transformer.split_query_key_value_and_split_heads(
        fused_qkv_output,
        memory_config=ttnn.L1_MEMORY_CONFIG,
        num_heads=num_heads,
    )
    ttnn.deallocate(fused_qkv_output)

    attention_scores = ttnn.matmul(
        query,
        key,
        memory_config=ttnn.L1_MEMORY_CONFIG,
        dtype=ttnn.bfloat16,
        core_grid=ttnn.CoreGrid(y=batch_size, x=num_cores_x),
    )
    ttnn.deallocate(query)
    ttnn.deallocate(key)

    attention_probs = ttnn.transformer.attention_softmax_(attention_scores, attention_mask=attention_mask, head_size=head_size)

    context_layer = ttnn.matmul(
        attention_probs,
        value,
        memory_config=ttnn.L1_MEMORY_CONFIG,
        dtype=ttnn.bfloat8_b,
        core_grid=ttnn.CoreGrid(y=batch_size, x=num_cores_x),
    )
    ttnn.deallocate(attention_probs)

    context_layer_after_concatenate_heads = ttnn.transformer.concatenate_heads(
        context_layer,
        memory_config=ttnn.L1_MEMORY_CONFIG,
    )
    ttnn.deallocate(context_layer)

    self_output = ttnn.linear(
        context_layer_after_concatenate_heads,
        self_output_weight,
        bias=self_output_bias,
        memory_config=ttnn.L1_MEMORY_CONFIG,
        dtype=ttnn.bfloat16,
        core_grid=ttnn.CoreGrid(y=batch_size, x=num_cores_x),
    )
    ttnn.deallocate(context_layer_after_concatenate_heads)

    return self_output
```

## Pre-process the parameters of the optimized model

1. Fuse QKV weights and biases
2. Reshape and tilize for the optimized operations using preprocess_linear_weight and preprocess_linear_bias
3. Move to device

```ipython3
from ttnn.model_preprocessing import (
    preprocess_linear_bias,
    preprocess_linear_weight,
)

torch_qkv_weight = torch.cat([torch_query_weight, torch_key_weight, torch_value_weight], dim=-1)
torch_qkv_bias = torch.cat([torch_query_bias, torch_key_bias, torch_value_bias], dim=-1)

qkv_weight = preprocess_linear_weight(torch_qkv_weight.T, dtype=ttnn.bfloat16)
qkv_bias = preprocess_linear_bias(torch_qkv_bias, dtype=ttnn.bfloat16)
output_weight = preprocess_linear_weight(torch_output_weight.T, dtype=ttnn.bfloat16)
output_bias = preprocess_linear_bias(torch_output_bias, dtype=ttnn.bfloat16)

qkv_weight = ttnn.to_device(qkv_weight, device)
qkv_bias = ttnn.to_device(qkv_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)
output_weight = ttnn.to_device(output_weight, device)
output_bias = ttnn.to_device(output_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)
```

## Run the first iteration of the optimized Multi-Head Attention

```ipython3
start = time.time()
hidden_states = ttnn.to_layout(hidden_states, ttnn.TILE_LAYOUT)
optimized_output = optimized_multi_head_attention(
    hidden_states,
    attention_mask,
    qkv_weight,
    qkv_bias,
    output_weight,
    output_bias,
    num_heads=num_heads,
)
end = time.time()
duration = end - start
```

```ipython3
print(f"Optimized multi-head attention ran in {duration} seconds for the first iteration")
```

<pre>
Optimized multi-head attention ran in 3.9824471473693848 seconds for the first iteration
</pre>

## Run a subsequent iteration of the optimized Multi-Head Attention

```ipython3
start = time.time()
optimized_output = optimized_multi_head_attention(
    hidden_states,
    attention_mask,
    qkv_weight,
    qkv_bias,
    output_weight,
    output_bias,
    num_heads=num_heads,
)
end = time.time()
duration = end - start
```

```ipython3
print(f"Optimized multi-head attention ran in {duration} seconds for the subsequent iteration because of the program cache")
```

<pre>
Optimized multi-head attention ran in 0.002396821975708008 seconds for the subsequent iteration because of the program cache
</pre>

Note that the optimized multi-head attention is 2 orders of magnitude faster than the initial version

## Check that the output of the optimized version matches the output of the original implementation

```ipython3
torch_output = ttnn.to_torch(output)
torch_optimized_output = ttnn.to_torch(optimized_output)

assert torch.allclose(torch_output, torch_optimized_output)
```

## Close the device

```ipython3
ttnn.close_device(device)
```

<pre>
<span style="color: rgb(0,128,0)">                  Metal</span> | <span class="ansi-bold" style="color: rgb(100,149,237)">INFO    </span> | Closing device 0
<span style="color: rgb(0,128,0)">                  Metal</span> | <span class="ansi-bold" style="color: rgb(100,149,237)">INFO    </span> | Disabling and clearing program cache on device 0
</pre>
